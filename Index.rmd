---
title: "Predicting Health Outcome (Classification Model)"
author: "Olamide Adu"
date: "2023-03-25"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    dev: svg
    theme: simplex
    highlight: zenburn
    code_folding: show
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

# Retrospective Health-data Analysis
Retrospective exploratory data analysis of intensive care unit health-data downloaded from kaggle.com already preprocessed by user @saurabhshahane.

**NB**: The [data](https://www.kaggle.com/datasets/saurabhshahane/in-hospital-mortality-prediction) used is part of a whole data from [**MIMIC III original database**](https://www.kaggle.com/datasets/drscarlat/mimic2-original-icu/download?datasetVersionNumber=1).

Mainly
![R](https://raw.githubusercontent.com/xrander/Health-data/master/Resource/4375063_logo_project_r_icon.png)
and
![](https://github.com/xrander/Health-data/raw/master/Resource/4691328_postgresql_icon.png)

## The Data
We load the libraries for reading and analyzing the data first.

```{r load_libraries, message=FALSE, results='hide'}
# Load the doBy library for advanced data manipulation and data aggregation
library('doBy')

library("reshape2")

# Load the tidyverse library for data manipulation and visualization
library('tidyverse')

# Load corrplot package for correlation matrix
library('corrplot')

# Load e1071 to impute missing values and svm model training
library("e1071")

# Random Forest
library("randomForest")

# For classification and regression training
library("caret")

# Recursive partitioning and regression tree plots
library("rpart")
library("reprtree")
library("rpart.plot")
```
Next we import the data.frame

```{r load_data, message = FALSE, cols.print = 15}
# Import the data
health_rec <- read_csv('https://raw.githubusercontent.com/xrander/Health-data/master/Health_data.csv')

# Preview the first ten rows of the data
head(health_rec, n = 10)
```

# Description of the Data

The MIMIC-III database is publicly a available critical care database containing de-identified data on 46520 patients and 58976 admissions to the ICU of the Beth Israel Deaconess Medical Center, Boston, USA between
2001 and 2008. The data used for this study is limited to a bit more than 1000 patient only.
![](https://news.mit.edu/sites/default/files/images/202207/MIT-Healthcare-Technology-01.jpg)

## Data Definition

-   group_num

-   ID: The patient ID

-   outcome: 0 = Alive and 1 = Dead

-   age: the age of the patient

-   gender: 1 = Male and 2 = Female

-   BMI: Body Mass Index

-   hypertensive: 0 = with and 1 = without

-   atrialfibrillation: 0 = with and 1 = without

-   CHD_with_no_MI: Coronary heart disease with Myocardial infarction, 0 = with and 1 without

-   diabetes: 0 = with and 1 without

-   deficiencyanemias: 0 = with and 1 = without

-   depression: 0 = with and 1 = without

-   Hyperlipemia:this refers to high concentration of fats or lipids in the blood. 0 = with and 1 = without

-   Renal_failure: 0 = without renal failure and 1 = with renal failure

-   COPD: chronic obstructive pulmonary disease(A disease causing airflow blockage and breathing related problems). 0 = without and 1 = without

-   heart_rate: a range of 60 to 100 bpm is considered normal

-   Systolic_blood_pressure: measure of the pressure in arteries when the heart beats.

-   Diastolic_blood_pressure: measure of the pressure in the arteries when the heart rests between beats

-   Respiratory_rate: this is the number of breathes per minute. The normal range is between 12-20, anything under above or below this range is abnormal. 25 is considered too high.

-   temperature

-   SP_O2: Oxygen saturation (measure of how much oxygen the blood carries as a percentage of the maximum it could carry). The normal range for healthy individuals is between 96% to 99%

-   Urine_output

-   hematocrit: percentage volume of red blood cells in the blood.

-   RBC: acronym for Red Blood Count.

-   MCH: Mean Corpuscular Hemoglobin, this is the average amount of hemoglobin in each of the red blood cells

-   MCHC: Mean Cell Hemoglobin Concentration, this is the average concentration of hemoglobin in a given volume of blood.

-   MCV: Mean Corpuscular Volume, this is the measure of the average size of the red clood cells

-   RDW: red cell distribution width. it is the differences in the volume and size of the red blood cells

-   Leucocyte

-   Platelets

-   Neutrophils

-   Basophils

-   Lymphocyte

-   PT: measure of the time it takes the liquid portion of the blood to clot.

-   INR

-   NT_proBNP: a measure of heart failure.

-   Creatine_kinase

-   Creatinine

-   Urea_nitrogen

-   glucose

-   Blood_potassium

-   Blood_sodium

-   Blood_calcium

-   Chloride

-   Anion_gap: measures the difference between the negatively and positively charged electrolytes in the blood.

-   Magnesium_ion

-   PH

-   Bicarbonate

-   Lactic_acid

-   PCO2: Partial Pressure of Carbon Dioxide, this measures the carbondioxide within the arterial or venous blood.

-   EF: Ejection Fraction: Used to gauge how healthy the heart is. It is the amount of blood that the heart pumps each time it beats.

# Data Preparation and Preprocessing

> **Note:** The PostgreSQL code solution to the descriptive statistic questions asked are
> [here](https://raw.githubusercontent.com/xrander/Health-data/master/Health_data_analysis.sql)

## Explanatory Variable and Dependent Variable
The dependent variable for this analysis is the outcome, while other variables are the explanatory variable.

##Understanding the Data

It is important we understand the data

### The Data Structure
```{r data_structure,message=FALSE}
# data structure
str(health_rec)
```
At first glance, all the variables are numeric type. There are `r dim(health_rec)[1]` observations and `r dim(health_rec)[2]` variables. Some of the variables of the data does not follow the data definition. A quick descriptive summary of the data shows this.

```{r data_stats}
# quick descriptive statistics of the data
summary(health_rec)
```

We can expect two data type according to the data description. The first is categorical variables for data which min is 0 and max is 1, and the other is numeric variables.

The descriptive statistics also shows the min and max age as `r min(health_rec$age)` and `r max(health_rec$age)`.

### Missing values
Next, we investigate the data for missing values. It is alright to have some missing values for some variables, but the outcome should be known, as it is important to see if patients are dead or alive. Outcome will be used to handle the missing data.
```{r remove_na}
# Remove missing data from the outcome variable
health_rec <- health_rec%>%
  filter(!is.na(outcome))
```

There is no missing data present in the outcome of patients. The data have been reduced from 1177 to `dim(health_rec)[1]`.
To deal with missing values from the other variables other than outcome, we need to know the missing value for the categorical and numeric variable and take the necessary steps.

#### Categorical Varaibles
```{r categorical_variables}
categorical <- health_rec %>%
  select(gender, hypertensive:COPD)

unique(is.na(categorical))
```
The categorical variables do not have missing values.

#### Numerical Variables distribution
For the numerical variable, the descriptive statistics carried out earlier shows that some numerical variables contains missing values
```{r numerical_variables}
numerical <- health_rec %>%
  select(-colnames(categorical), -c(1:4))

str(numerical)
```
We are going to impute the missing values in the numerical variables using their mean. We use the e1071 package **impute()** function to impute the means here.

```{r}
imputed_means <- numerical %>%
  impute(what = "mean")
```

##### Distibution of Numerical Variables
The new data will be tidied to visualize the distribution of the numerical variables.
```{r}
as_tibble(imputed_means) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "value") %>%
  ggplot(show.legend = F)+
  geom_density(aes(value), col = "tomato2")+
  facet_wrap(~variable, scales = "free")+
  theme_minimal()
```
###### Normally distributed Variables
Variables provides information on the body's electrolyte and Mineral content such as anion gap, bicarbonate, blood calcium, blood potassium, blood sodium and chloride are normally distributed. Vital signs and Cardiovascular parameters such as temperature, PH, Diastolic blood pressure, and heart rate are also normally distributed. Only three Hematology parameters, MCH, MCHC and MCV are normally distributed.

###### Right-skewed Variables
Respiratory rate, RBW, RBC, urea nitrogen, urine output, basophils, BMI, creatine-Kinase, creatinine, glucose, hematocrit, INR, lactic acid, leucocyte, lymphocyte, magnesium ion, NT Pro BMP, PCO2, platelets, PT, and systolic blood pressure are all right skewed.

###### Left-skewed Variables
Only three variables are left skewed. These includes neutrophils, EF, and SP O2.

#### Imputing Missing Values

Next we replace the old values with new ones in the main data frame. In the process, the group_num and ID column were also removed.
```{r}
health_rec_clean <- health_rec %>%
  select(-c(1,2),-c(names(numerical))) %>% # remove unwanted variables and old numerical variables
  bind_cols(imputed_means) %>% # add imputed values columns to the data frame
  mutate(id = row_number()) %>% # add row numbers to the data frame
  select(id, everything()) # rearrange variables to make id come first

tail(health_rec_clean, 10)
```


Now we check to find if there are still missing values.
```{r}
unique(is.na(health_rec_clean))
```

All numerical variable missing values are replaced by their means.

# Exploratory Data Analysis
A majority of the question asked are descriptive statistics questions and answers will be provided using visualization and tables.

## Descriptive Statistics
### What is the percentage age distribution according to age groups
**The age group with the highest frequency**: To do this, we have to bin the age variable, creating a class range, then we can create a summary of the age_group and see their frequency
```{r age-group-class-interval}
# Create bins
age_categories <- seq(0, 120, by = 20) # We assume the highest age is not above 120 years old

# Create age interval variable
health_rec_clean <- health_rec_clean %>%
  mutate(age_group = cut(age,
                         breaks = age_categories,
                         include.lowest = T, # To indicate if x is the lowest
                         ordered_result = F),#This ensures that the factor variable is not ordered
         age_group =  ifelse(age_group == "[0,20]", "0-20",
                             ifelse(age_group == "(20,40]", "20-40",
                                    ifelse(age_group == "(40,60]", "40-60",
                                           ifelse(age_group == "(60,80]", "60-80", "80-100")))), # Change the group names to ensure consistency
         age_group = factor(age_group, levels = c("0-20", "20-40","40-60", "60-80", "80-100")))
```


```{r age-group-frequency-visualization}
health_rec_clean %>%
  group_by(age_group) %>%
  summarize(frequency = n()) %>%
  ggplot(aes("", frequency, fill = age_group))+
  geom_col(color = "white")+
  geom_text(aes(label = paste0(round(frequency/sum(frequency)*100, 1), "%")),
              position = position_stack(vjust = 0.5))+
  scale_fill_manual(values = c("pink", "coral", "tomato1", "orangered1", "indianred1"))+
  labs(x = NULL,
       y = NULL,
       fill = "Age Group",
       title = "Pie Chart of Age Groups")+
  coord_polar(theta = "y", start = 0)+
  theme(legend.position = "top",
        axis.line = element_blank(),
        plot.background = element_blank(),
        rect = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())
```

Older people are the highest in the hospital, with people between 60 - 80 years old having being the highest in the clinic and people more than 80 years old being the next most occurring patients.
To know the age(not age group) that is most occurring, we can simply group by the age them.

### Top Occuring ages in the Hospital
```{r message=FALSE}
health_rec_clean %>%
  group_by(age) %>%
  summarize(frequency = n()) %>%
  arrange(desc(frequency)) %>%
  top_n(n = 6)
```
The highest count of patients in the clinic are aged 89, 84 and 81 years respectively.

### What is the number of dead patients for each gender*
```{r message=FALSE}
health_rec_clean %>%
  filter(outcome == 1) %>%
  group_by(gender, age_group) %>%
  summarize(dead_patients = length(outcome)) %>%
  arrange(desc(dead_patients)) %>%
  ggplot(aes(age_group, dead_patients, fill = factor(gender))) +
  geom_col(position = "dodge")+
  scale_fill_manual(values = c("tomato2", "thistle4"),
                    labels = c("Male", "Female"))+
  labs(x = "Age group",
       y = "Dead patients",
       fill = "Gender",
       title = "Dead counts across the different gender")+
  expand_limits(y = c(0, 40))+
  theme_bw()
```

The chart shows the dead patients according to age group and gender. The age group 80-100 years for females have the highest count of death.

### Gender Frequency
The gender with the highest count in the clinic is the female gender as show below
```{r frequency-of-gender}
health_rec_clean %>%
  select(gender) %>%
  mutate(gender = ifelse(gender == 1, "Male", "Female")) %>%
  group_by(gender) %>%
  count() %>%
  ggplot(aes(gender, n))+
  geom_col(fill = c("tan3", "wheat4"))+
  ggtitle("Frequency of Patients According to Their Gender")+
  theme_bw()
```

#what is the death rate of both gender for a given age group?

```{r death-rate-for-age-group}
health_rec_clean %>%
  mutate(outcome = ifelse(outcome == 0, "Alive", "Dead"),
         gender = ifelse(gender == 1, "Male", "Female")) %>%
  group_by(gender, outcome, age_group) %>%
  count() %>%
  pivot_wider(names_from = outcome,
              values_from = n,
              values_fill = 0) %>%
  mutate(total = sum(Alive, Dead),
         death_per_100 = Dead/100,
         death_ratio = round(Dead/total, 2))
```

### Disease and Death
Now we will investigate the 159 death occurrences that is present in the clinic and relate it to some of the diseases.

```{r}
dead_patients <- health_rec_clean %>%
  filter(outcome == 1) %>%
  select(outcome:COPD)
```

**Tidying the data for visualization**
```{r}
dead_patients_tidy <- dead_patients %>%
  pivot_longer(cols = c(hypertensive:COPD),
               names_to = "diseases",
               values_to = "state") %>%
  mutate(outcome = ifelse(outcome == 0, "Alive", "Dead"),
         gender = ifelse(gender == 1, "Male", "Female"),
         state = ifelse(diseases == "Renal_failure", "Not having", "Having"),
         state = ifelse(diseases != "Renal_failure", "Having", "Not Having"))
```

```{r}
dead_patients_tidy %>%
  ggplot(aes(state, fill = gender))+
  geom_bar(position = "dodge")+
  scale_fill_manual(values = c("wheat4", "tan3"))+
  facet_wrap(~diseases, scales = "free_y")+
  theme_bw()
```
All dead patients seems to be having a disease or health condition, while Renal_failure have not been related to death.

##  Correlation Analysis
```{r}
correlation_plot <- cor(health_rec_clean[,c(1:2, 13:49)])

ggplot(melt(correlation_plot), aes(Var1, Var2, fill = value, label = round(value,2)))+
  geom_tile()+
  geom_text(aes(label= ifelse(value>0.1, as.character(round(value, 1)), "")))+
  scale_fill_gradient(low = "tomato",
                      high = "skyblue")+
  labs(title = "Correlation plot of variables")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



# Outcome Prediction
Machine Learning Algorithms to be used
We are going to predict the outcome of any one patients making use of four machine learning algorithms viz:

- Logistic Regression

- Random Forest Classifier

- Support Vector Machines (SVMs)

- Gradient Boosting Algorithms (XGboosts)

Models will be evaluated using:

- ROC curve

- Confusion Matrix

- Precision scores
While these are used, Regression related metrics such as MAE, MSE, RMSE and R-squared will also be used based on the algorithm.

## Data Splitting
Split the data and set seed.
```{r}
# Set seed to ensure reproducibility
set.seed(1599)

# Split the data to train and test data
training_data_index <- createDataPartition(health_rec_clean$outcome,
                    p = 0.8,
                    list = F)

# Training data
health_rec_train <- health_rec_clean[training_data_index, ]
# Test data
health_rec_test <- health_rec_clean[-training_data_index,]
```

```{r set train and test data, message=FALSE}
# Remove id number and age_group columns
health_rec_train <- health_rec_train %>%
  select(-c(1, 51))

health_rec_test <- health_rec_test %>%
  select(-c(1, 51))
```

**Preview split data**
```{r}
head(health_rec_test, n = 5)

head(health_rec_train, n = 5)
```

## Model Building and Tuning

### Logistic Regression
####Training the model
```{r logistic_regression, message=FALSE}
logistic_model <- glm(formula = outcome ~ .,
                      family = "binomial",
                      data = health_rec_train)
```

Ten variables blood calcium,  urea nitrogen, creatinine, platelets, leucocytes, PCO2, heart rate, diastolic blood pressure, COPD, and renal failure have a significant effect on the outcome.

#### Making Predictions
```{r}
logistic_pred <- ifelse(predict(logistic_model, health_rec_test) >= 0.5, 1, 0)
```

```{r}
mean(health_rec_test$outcome == logistic_pred)
```
The logistic model is having a 88.1% accuracy.

### Random Forest Classifier
Random forest classifier uses multiple decision tree models and try to mitigate the overfitting from a single decision tree by aggregating the decision from multiple decision trees.
#### Hypertuning parameter for Random Forest
```{r hyperparameter-tuning, message=FALSE, warning=FALSE}
# Create parameter grid for search values

param_grid <- expand.grid(mtry = c(50, 200, 300,500, 700))

# define control function for tuning
control <- trainControl(method = "cv", number = 5) # Use 5 folds cross-validation

model <- train(factor(outcome) ~ ., data = health_rec_train, method = "rf", trControl = control, tuneGrid = param_grid)
print(model)
```

#### Training the Random Forest Model
The best parameter for ntree = 300 will be used for the model.
```{r random-forest.model, message=FALSE, warning=FALSE}
rf_model <- randomForest(outcome ~ .,
                         data = health_rec_train,
                         ntree = 300)
```


#### Predicting the outcome
```{r}
rf_pred <- ifelse(predict(rf_model, health_rec_test, type = "response") >= 0.5, 1, 0)
```

```{r}
mean(health_rec_test$outcome == rf_pred)
```

The model is having 87.6% accuracy a bit lower than the logistic regression model.

### Support Vector Machines (SVM)
```{r}
health_rec_train2 <- health_rec_train %>%
  mutate(outcome = factor(outcome))
svm_model <- svm(outcome ~ .,
                 data = health_rec_train2,
                 kernel = "linear")

summary(svm_model)
```
#### Predicting with the outcome with SVM
```{r}
svm_pred <- predict(svm_model, health_rec_test2)
```


```{r}
mean(health_rec_test2$outcome == svm_pred)
```
The model is having 88.1% accuracy, similar to logistic regression


### XGBoost
Next we will use the xgboost package
```{r load-xgboost-library, message=FALSE}
#load the xgboost library
library(xgboost)
```

#### XGBoost data Preparation
We create the xgboost data type
```{r}
# Create XGBosst data structure for test and train data
dtrain <- xgb.DMatrix(data = as.matrix(health_rec_train[, -1]), label = as.matrix(health_rec_train[, 1]))
dtest <- xgb.DMatrix(data = as.matrix(health_rec_test[, -1]))
```

#### Training the Data
Next we train the data
```{r results='hide'}
# Set parameter structure
params <- list(
  objective = "binary:logistic",
  max_depth = 10,
  eval_metric = "logloss"
)
# Train the XGBoost model
xgb_model <- xgboost(params = params,
                     data = dtrain,
                     nthread = 2,
                     nrounds = 200)
```

#### Predicting with the Model
Now we can make predictions
```{r}
xgboost_pred <- ifelse(predict(xgb_model, dtest, type = "class") >=0.5, 1, 0)
```

```{r}
mean(xgboost_pred == health_rec_test$outcome)
```
The model is having a 90% accuracy which is the best performance so far.

## Model Evaluation
The confusion matrix is used as the only evaluation method for this study.
### Logistic Regression
```{r}
table(log_predicted = logistic_pred, actual = health_rec_test$outcome)
```

The model performs poorly at predicting dead individuals. It is having more false positives, predicting that some patients are alive instead dead.

### Random Forests Classifier
```{r}
table(random_forest_predicted = rf_pred, actual = health_rec_test$outcome)
```

This performance is also similar to logistic model, it is also having more false positives, predicting that some individuals are dead instead of alive.

### SVM
```{r}
table(prediction = svm_pred, actual = health_rec_test2$outcome)
```

### XGBoosts
```{r}
table(xgboost_pred, health_rec_test$outcome)
```
Predictions by XGBoost is the best so far, it performs better in predicting actual death than other models

# Conclusion
Four models, XGBoost, logistic regression, SVM and random forest,  were tested to see which is having the best outcome. XGBoost is having the best prediction with 90% probability of success.

[Back to Homepage](https://olamideadu.com)
